#!/bin/bash
function wait_for() {
  local cmdstr=$@
  local total_attempts=360
  local sleep_time=10
  echo "[NOTE] Waiting for cmd to return success: ${cmdstr}"
  for attempt in $(seq "${total_attempts}"); do
    eval "${cmdstr}" && break || true
    echo -n '.'; sleep "${sleep_time}"
  done
}

# Wait for MaaS commissioning/deploy to finish, retry on failure
function maas_fixup() {
  local statuscmd="salt 'mas01*' --out yaml state.apply maas.machines.status"
  wait_for "${statuscmd} | tee /dev/stderr | " \
           "grep -Eq '((Deployed|Ready): 5|status:Failed|status:Allocated)'"

  statusout=$(eval "${statuscmd}")
  # sample output (remove this when implementation is done):
  # - hostname:cmp002,system_id:wgq4dp,status:Deployed
  # - hostname:cmp001,system_id:gfm64c,status:Failed deployment

  if echo ${statusout} | grep -q "status:Failed commissioning"; then
    # TODO: catch system_id for failed nodes using grep on ${statusout}
    # TODO(optional): implement machine delete in _modules/maas.py
    # maas login opnfv ...
    # maas opnfv machine delete gfm64c
    salt -C 'mas01*' state.apply maas.machines
    return 1
  fi
  if echo ${statusout} | grep -q "status:(Failed deployment|Allocated)"; then
    # TODO: catch system_id for failed nodes using grep on ${statusout}
    # TODO(optional): implement machine mark-{broken,fixed} in _modules/maas.py
    # maas login opnfv ...
    # maas opnfv machine mark-broken gfm64c
    # maas opnfv machine mark-fixed gfm64c
    salt -C 'mas01*' state.apply maas.machines.deploy
    return 1
  fi

  return 0
}

# MaaS rack/region controller, node commissioning
salt -C 'mas01*' cmd.run "add-apt-repository ppa:maas/stable"

salt -C 'mas01*' state.apply linux,salt,openssh,ntp
salt -C 'mas01*' state.apply linux.network.interface
salt -C 'mas01*' state.apply maas.pxe_nat
salt -C 'mas01*' state.apply maas.cluster
salt -C 'cfg01*' cmd.run \
  "route add -net 192.168.11.0/24 gw ${MAAS_IP:-192.168.10.3}"

wait_for "salt -C 'mas01*' state.apply maas.region"

salt -C 'mas01*' state.apply maas.machines
wait_for maas_fixup

# cleanup outdated salt keys
salt-key --out yaml | awk '!/^(minions|- cfg01|- mas01)/ {print $2}' | xargs -I{} salt-key -yd {}

# MaaS node deployment
salt -C 'mas01*' state.apply maas.machines.deploy
wait_for maas_fixup

# on failure, mark broken, mark fixed, deploy again


salt -C 'mas01*' pillar.item\
  maas:region:admin:username \
  maas:region:admin:password

# KVM, compute node prereqs (libvirt first), VCP deployment
salt -C '* and not cfg01* and not mas01*' saltutil.sync_all

salt -C 'kvm*' pkg.install bridge-utils
salt -C 'kvm*' state.apply linux.network
salt -C 'kvm*' system.reboot
wait_for "! salt '*' test.ping | tee /dev/stderr | fgrep -q 'Not connected'"

salt -C '* and not cfg01* and not mas01*' state.apply linux,ntp

salt -C 'kvm*' state.sls libvirt

salt -C '* and not cfg01* and not mas01*' state.apply salt
salt -C 'kvm*' saltutil.sync_all
salt -C 'kvm*' state.sls salt.control

vcp_nodes=$(salt --out yaml 'kvm01*' pillar.get salt:control:cluster:internal:node | awk '/\s+\w+:$/ {gsub(/:$/, "*"); print $1}')

# Check all vcp nodes are available
rc=1
while [ $rc -ne 0 ]; do
  rc=0
  for node in $vcp_nodes; do
    salt "$node" test.ping 2>/dev/null || { rc=$?; break; };
  done
  sleep 5
done

salt -C '* and not cfg01* and not mas01*' saltutil.sync_all
salt -C '* and not cfg01* and not mas01*' state.apply salt
wait_for "salt -C '* and not cfg01* and not mas01*' state.apply linux,ntp"
